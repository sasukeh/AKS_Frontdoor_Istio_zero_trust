# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Azure Front Door + Istio + AKSã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ˆãç™ºç”Ÿã™ã‚‹å•é¡Œã¨ãã®è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ†• 2025å¹´7æœˆ: å®Ÿè£…ã§ç™ºè¦‹ã•ã‚ŒãŸé‡è¦ãªå•é¡Œã¨è§£æ±ºç­–

### Critical Problem 1: Front Door 404ã‚¨ãƒ©ãƒ¼ï¼ˆPrivate Link Serviceæ¥ç¶šæœªæ‰¿èªï¼‰

**ç—‡çŠ¶**: 
- Front Doorã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã§404 Not FoundãŒè¿”ã•ã‚Œã‚‹
- ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã¯æ­£å¸¸ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹
- AKSå†…éƒ¨ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã¯æˆåŠŸã™ã‚‹

**æ ¹æœ¬åŸå› **: 
Front Doorã‹ã‚‰Private Link Serviceã¸ã®æ¥ç¶šãŒè‡ªå‹•æ‰¿èªã•ã‚Œãšã€PendingçŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹

**è¨ºæ–­ã‚³ãƒãƒ³ãƒ‰**:
```bash
# Private Link Serviceæ¥ç¶šçŠ¶æ³ç¢ºèª
az network private-link-service show -g <resource-group> -n <pls-name> \
  --query "privateEndpointConnections[].privateLinkServiceConnectionState" --output table

# æœŸå¾…ã•ã‚Œã‚‹çµæœ: Status ãŒ "Approved"
# å•é¡ŒãŒã‚ã‚‹å ´åˆ: Status ãŒ "Pending"
```

**è§£æ±ºæ–¹æ³•**:
```bash
# 1. æ¥ç¶šã®è©³ç´°æƒ…å ±å–å¾—
az network private-link-service show -g <resource-group> -n <pls-name> \
  --query "privateEndpointConnections[].{name:name,status:privateLinkServiceConnectionState.status}" --output table

# 2. å„æ¥ç¶šã‚’æ‰‹å‹•æ‰¿èªï¼ˆã™ã¹ã¦ã®Pendingæ¥ç¶šã«å¯¾ã—ã¦å®Ÿè¡Œï¼‰
az network private-link-service connection update \
  -g <resource-group> \
  --service-name <pls-name> \
  --name <connection-name> \
  --connection-status Approved \
  --description "Approved Front Door connection"

# 3. æ‰¿èªå¾Œ1-2åˆ†å¾…ã£ã¦ã‹ã‚‰ãƒ†ã‚¹ãƒˆ
curl -k https://<frontdoor-endpoint>
```

### Critical Problem 2: Istio Pods Pendingï¼ˆNode Taintæœªå¯¾å¿œï¼‰

**ç—‡çŠ¶**:
- `kubectl get pods -n istio-system` ã§PodãŒPendingçŠ¶æ…‹
- Events ã« `0/2 nodes are available: 2 node(s) had untolerated taints`

**æ ¹æœ¬åŸå› **:
AKSãƒãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã®Taintè¨­å®šã«Istio PodsãŒå¯¾å¿œã—ã¦ã„ãªã„

**è¨ºæ–­ã‚³ãƒãƒ³ãƒ‰**:
```bash
# ãƒãƒ¼ãƒ‰ã®Taintç¢ºèª
kubectl describe nodes | grep -A5 Taints

# Istio Podã®çŠ¶æ…‹ç¢ºèª
kubectl describe pod -n istio-system <istio-pod-name>
```

**è§£æ±ºæ–¹æ³•**:
Istioè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`istio-install-config-fixed.yaml`ï¼‰ã«é©åˆ‡ãªtolerationã‚’è¿½åŠ :
```yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  components:
    pilot:
      k8s:
        tolerations:
          - key: "workload"
            operator: "Equal"
            value: "user" 
            effect: "NoSchedule"
          - key: "CriticalAddonsOnly"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        nodeSelector:
          agentpool: user
    ingressGateways:
      - name: istio-ingressgateway
        enabled: true
        k8s:
          tolerations:
            - key: "workload"
              operator: "Equal"
              value: "user"
              effect: "NoSchedule"
            - key: "CriticalAddonsOnly"  
              operator: "Equal"
              value: "true"
              effect: "NoSchedule"
          nodeSelector:
            agentpool: user
```

### Critical Problem 3: Terraform Location Mismatch

**ç—‡çŠ¶**:
```
Error: InvalidResourceReference: Private link service cannot reference frontend ip configuration since it is already referenced
```

**æ ¹æœ¬åŸå› **:
- å¤‰æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒå®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆã¨ç•°ãªã‚‹
- "Japan East" vs "southeastasia" ã®ä¸ä¸€è‡´

**è§£æ±ºæ–¹æ³•**:
```hcl
# terraform/main.tf ã§locationã‚’æ˜ç¤ºçš„ã«æ¸¡ã™
module "frontdoor" {
  source = "./modules/frontdoor"
  location = var.location  # ã“ã®è¡Œã‚’è¿½åŠ 
  # ...ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
}
```

### Critical Problem 4: Load Balancer Frontend IP Configuration ç‰¹å®š

**ç—‡çŠ¶**:
Private Link Serviceã§Load Balancerã®frontendIPConfiguration IDãŒä¸æ˜

**è§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³**:
AKSãŒç”Ÿæˆã™ã‚‹ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰IPè¨­å®šåã¯ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³:
```
{hash}-{subnet-name}
```

**ç¢ºèªæ–¹æ³•**:
```bash
# Load Balancerç¢ºèª
kubectl get services -n istio-system istio-ingressgateway

# Azure CLIã§è©³ç´°ç¢ºèª  
az network lb show -g MC_<resource-group>_<cluster-name>_<location> -n kubernetes-internal
```

## ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–¢é€£ã®å•é¡Œ

### 1. Terraformãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼

#### 1.1 èªè¨¼ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `Error: building account: could not acquire access token`

**åŸå› **: Azureèªè¨¼ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ãªã„

**è§£æ±ºç­–**:
```bash
# Azure CLIã§ãƒ­ã‚°ã‚¤ãƒ³
az login

# ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ç¢ºèª
az account show

# å¿…è¦ã«å¿œã˜ã¦ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
az account set --subscription "your-subscription-id"
```

#### 1.2 ãƒªã‚½ãƒ¼ã‚¹åã®é‡è¤‡
**ç—‡çŠ¶**: `A resource with the ID already exists`

**åŸå› **: æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹åã¨é‡è¤‡ã—ã¦ã„ã‚‹

**è§£æ±ºç­–**:
```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒªã‚½ãƒ¼ã‚¹åã‚’å¤‰æ›´
RESOURCE_GROUP_NAME="rg-frontdoor-istio-demo-$(date +%Y%m%d)"
AKS_CLUSTER_NAME="aks-frontdoor-istio-$(date +%Y%m%d)"
```

#### 1.3 ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `Quota exceeded` ã¾ãŸã¯ `SKU not available`

**åŸå› **: ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³åˆ¶é™ã¾ãŸã¯ãƒªãƒ¼ã‚¸ãƒ§ãƒ³åˆ¶é™

**è§£æ±ºç­–**:
```bash
# åˆ©ç”¨å¯èƒ½ãªVMã‚µã‚¤ã‚ºç¢ºèª
az vm list-sizes --location japaneast

# ã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèª
az vm list-usage --location japaneast

# åˆ¥ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã¾ãŸã¯VMã‚µã‚¤ã‚ºã‚’è©¦è¡Œ
LOCATION="eastus"
AKS_NODE_VM_SIZE="Standard_D2s_v3"
```

### 2. AKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ¥ç¶šå•é¡Œ

#### 2.1 kubeconfigå–å¾—ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `ERROR: (AuthorizationFailed)`

**åŸå› **: AKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®æ¨©é™ä¸è¶³

**è§£æ±ºç­–**:
```bash
# Azure ADæ¨©é™ç¢ºèª
az role assignment list --assignee $(az account show --query user.name -o tsv)

# AKSç®¡ç†è€…æ¨©é™å–å¾—
az aks get-credentials \
    --resource-group $RESOURCE_GROUP_NAME \
    --name $AKS_CLUSTER_NAME \
    --admin
```

#### 2.2 Private Clusterã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `Unable to connect to the server`

**åŸå› **: Private Clusterã®API Serverã«å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹

**è§£æ±ºç­–**:
```bash
# ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹
# ã¾ãŸã¯ã€Azure Bastionã‚„ã‚¸ãƒ£ãƒ³ãƒ—ãƒœãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨

# ä¸€æ™‚çš„ãªè§£æ±ºç­–ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
az aks update \
    --resource-group $RESOURCE_GROUP_NAME \
    --name $AKS_CLUSTER_NAME \
    --api-server-authorized-ip-ranges $(curl -s ifconfig.me)/32
```

## Istioé–¢é€£ã®å•é¡Œ

### 3. Istio Gatewayå•é¡Œ

#### 3.1 Istio GatewayãŒèµ·å‹•ã—ãªã„
**ç—‡çŠ¶**: `CrashLoopBackOff` ã¾ãŸã¯ `Pending`

**è¨ºæ–­ã‚³ãƒãƒ³ãƒ‰**:
```bash
# ãƒãƒƒãƒ‰çŠ¶æ…‹ç¢ºèª
kubectl get pods -n istio-system -l app=istio-ingressgateway

# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl describe pod -n istio-system -l app=istio-ingressgateway

# ãƒ­ã‚°ç¢ºèª
kubectl logs -n istio-system -l app=istio-ingressgateway
```

**ä¸€èˆ¬çš„ãªåŸå› ã¨è§£æ±ºç­–**:

1. **ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³**
   ```bash
   # ãƒãƒ¼ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
   kubectl top nodes
   kubectl describe nodes
   
   # ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚é‡èª¿æ•´
   kubectl patch deployment istio-ingressgateway -n istio-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy","resources":{"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}'
   ```

2. **LoadBalancerä½œæˆã‚¨ãƒ©ãƒ¼**
   ```bash
   # ServiceçŠ¶æ…‹ç¢ºèª
   kubectl describe svc istio-ingressgateway -n istio-system
   
   # Azure Load Balancerç¢ºèª
   az network lb list --resource-group $NODE_RESOURCE_GROUP
   ```

#### 3.2 External IPå–å¾—ã§ããªã„
**ç—‡çŠ¶**: `<pending>` çŠ¶æ…‹ãŒç¶šã

**è¨ºæ–­**:
```bash
# Serviceè©³ç´°ç¢ºèª
kubectl describe svc istio-ingressgateway -n istio-system

# Azure Load Balancerç¢ºèª
NODE_RESOURCE_GROUP=$(az aks show --resource-group $RESOURCE_GROUP_NAME --name $AKS_CLUSTER_NAME --query nodeResourceGroup -o tsv)
az network lb list --resource-group $NODE_RESOURCE_GROUP
```

**è§£æ±ºç­–**:
```bash
# Internal Load Balancerã¨ã—ã¦è¨­å®š
kubectl patch svc istio-ingressgateway -n istio-system -p '{"metadata":{"annotations":{"service.beta.kubernetes.io/azure-load-balancer-internal":"true"}}}'
```

### 4. mTLSèªè¨¼å•é¡Œ

#### 4.1 ã‚µãƒ¼ãƒ“ã‚¹é–“é€šä¿¡ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `503 Service Unavailable` ã¾ãŸã¯ connection timeout

**è¨ºæ–­**:
```bash
# PeerAuthenticationç¢ºèª
kubectl get peerauthentication -A

# DestinationRuleç¢ºèª
kubectl get destinationrule -A

# Envoyè¨­å®šç¢ºèª
kubectl exec -n default <pod-name> -c istio-proxy -- curl localhost:15000/config_dump
```

**è§£æ±ºç­–**:
```bash
# mTLSè¨­å®šç¢ºèª
kubectl describe peerauthentication default -n istio-system

# å¿…è¦ã«å¿œã˜ã¦mTLSã‚’Permissiveãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
kubectl patch peerauthentication default -n istio-system --type merge -p '{"spec":{"mtls":{"mode":"PERMISSIVE"}}}'
```

## ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£ã®å•é¡Œ

### 5. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¨ãƒ©ãƒ¼

#### 5.1 ã‚¤ãƒ¡ãƒ¼ã‚¸Pullã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: `ImagePullBackOff` ã¾ãŸã¯ `ErrImagePull`

**è¨ºæ–­**:
```bash
kubectl describe pod <pod-name>
kubectl get events --sort-by='.lastTimestamp'
```

**è§£æ±ºç­–**:
```bash
# ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã®å ´åˆ
kubectl patch deployment frontend -p '{"spec":{"template":{"spec":{"containers":[{"name":"frontend","image":"nginx:alpine","imagePullPolicy":"Always"}]}}}}'

# ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®å ´åˆ
kubectl create secret docker-registry acr-secret \
    --docker-server=<acr-name>.azurecr.io \
    --docker-username=<service-principal-id> \
    --docker-password=<service-principal-password>
```

#### 5.2 Health Checkå¤±æ•—
**ç—‡çŠ¶**: Readiness Probeå¤±æ•—

**è¨ºæ–­**:
```bash
# ãƒãƒƒãƒ‰è©³ç´°ç¢ºèª
kubectl describe pod <pod-name>

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ç¢ºèª
kubectl logs <pod-name>

# æ‰‹å‹•ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
kubectl exec <pod-name> -- curl localhost:80/health
```

## Front Dooré–¢é€£ã®å•é¡Œ

### 6. Front Doorç–é€šå•é¡Œ

#### 6.1 502 Bad Gateway
**ç—‡çŠ¶**: Front DoorçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«502ã‚¨ãƒ©ãƒ¼

**è¨ºæ–­æ‰‹é †**:
```bash
# Front Doorè¨­å®šç¢ºèª
az cdn endpoint show --name $FRONTDOOR_ENDPOINT_NAME --profile-name $FRONTDOOR_PROFILE_NAME --resource-group $RESOURCE_GROUP_NAME

# ã‚ªãƒªã‚¸ãƒ³çŠ¶æ…‹ç¢ºèª
az cdn origin show --endpoint-name $FRONTDOOR_ENDPOINT_NAME --profile-name $FRONTDOOR_PROFILE_NAME --resource-group $RESOURCE_GROUP_NAME --name aks-origin
```

**ä¸€èˆ¬çš„ãªåŸå› **:
1. **Private Endpointè¨­å®šãƒŸã‚¹**
   ```bash
   # Private Endpointç¢ºèª
   az network private-endpoint list --resource-group $RESOURCE_GROUP_NAME
   ```

2. **Certificateå•é¡Œ**
   ```bash
   # TLSè¨­å®šç¢ºèª
   az cdn custom-domain show --endpoint-name $FRONTDOOR_ENDPOINT_NAME --profile-name $FRONTDOOR_PROFILE_NAME --resource-group $RESOURCE_GROUP_NAME --name custom-domain
   ```

#### 6.2 WAF Block
**ç—‡çŠ¶**: 403 Forbidden ã‚¨ãƒ©ãƒ¼

**è¨ºæ–­**:
```bash
# WAFãƒ­ã‚°ç¢ºèª
az monitor activity-log list --resource-group $RESOURCE_GROUP_NAME --max-events 50

# WAFãƒ«ãƒ¼ãƒ«ç¢ºèª
az network front-door waf-policy rule list --policy-name $WAF_POLICY_NAME --resource-group $RESOURCE_GROUP_NAME
```

**è§£æ±ºç­–**:
```bash
# ä¸€æ™‚çš„ã«WAFã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
az network front-door waf-policy update --name $WAF_POLICY_NAME --resource-group $RESOURCE_GROUP_NAME --mode Detection
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ

### 7. å¿œç­”æ™‚é–“ã®é…å»¶

#### 7.1 é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
**è¨ºæ–­**:
```bash
# Istio ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
kubectl exec -n istio-system <istio-proxy-pod> -- curl localhost:15000/stats/prometheus

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
kubectl top pods
kubectl top nodes
```

**æœ€é©åŒ–**:
```bash
# Connection Poolè¨­å®š
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: frontend-dr-optimized
spec:
  host: frontend
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
        h2MaxRequests: 100
EOF
```

### 8. ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³

#### 8.1 ãƒãƒ¼ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³
**ç—‡çŠ¶**: Pods ãŒ `Pending` çŠ¶æ…‹

**è¨ºæ–­**:
```bash
kubectl describe node
kubectl top nodes
kubectl get pods --all-namespaces | grep Pending
```

**è§£æ±ºç­–**:
```bash
# ãƒãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
az aks nodepool scale \
    --resource-group $RESOURCE_GROUP_NAME \
    --cluster-name $AKS_CLUSTER_NAME \
    --name user \
    --node-count 5
```

## ç·Šæ€¥æ™‚ã®å¯¾å¿œ

### 9. ç·Šæ€¥æ™‚ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### 9.1 ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãƒ€ã‚¦ãƒ³
**å¯¾å¿œæ‰‹é †**:
1. **ç¾çŠ¶ç¢ºèª**
   ```bash
   kubectl get pods --all-namespaces
   kubectl get nodes
   kubectl get svc
   ```

2. **ãƒ­ã‚°åé›†**
   ```bash
   kubectl logs --previous <pod-name>
   kubectl describe node <node-name>
   ```

3. **ç·Šæ€¥å¾©æ—§**
   ```bash
   # ãƒãƒƒãƒ‰å†èµ·å‹•
   kubectl rollout restart deployment/frontend
   kubectl rollout restart deployment/backend
   
   # Istio Gatewayå†èµ·å‹•
   kubectl rollout restart deployment/istio-ingressgateway -n istio-system
   ```

#### 9.2 ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †
```bash
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
kubectl rollout undo deployment/frontend
kubectl rollout undo deployment/backend

# Istioè¨­å®šãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
kubectl delete -f kubernetes/istio/
kubectl apply -f kubernetes/istio/previous-config/
```

## ãƒ­ã‚°åé›†ã¨ãƒ‡ãƒãƒƒã‚°

### 10. è©³ç´°ãƒ­ã‚°åé›†

#### 10.1 åŒ…æ‹¬çš„ãªãƒ­ã‚°åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# debug-collect.sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DEBUG_DIR="debug_${TIMESTAMP}"
mkdir -p $DEBUG_DIR

# KubernetesçŠ¶æ…‹
kubectl get all --all-namespaces > $DEBUG_DIR/k8s_all.txt
kubectl describe nodes > $DEBUG_DIR/nodes_describe.txt
kubectl get events --all-namespaces --sort-by='.lastTimestamp' > $DEBUG_DIR/events.txt

# IstioçŠ¶æ…‹
kubectl get gateway,virtualservice,destinationrule,peerauthentication -A > $DEBUG_DIR/istio_config.txt
kubectl logs -n istio-system -l app=istio-ingressgateway > $DEBUG_DIR/istio_gateway.log

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
kubectl logs -l app=frontend > $DEBUG_DIR/frontend.log
kubectl logs -l app=backend > $DEBUG_DIR/backend.log

# Azure ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹
az resource list --resource-group $RESOURCE_GROUP_NAME > $DEBUG_DIR/azure_resources.json

echo "Debug information collected in $DEBUG_DIR"
```

## äºˆé˜²ç­–ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 11. ç›£è¦–è¨­å®š

#### 11.1 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
```bash
# Azure Monitor ã‚¢ãƒ©ãƒ¼ãƒˆä½œæˆä¾‹
az monitor metrics alert create \
    --name "AKS CPU High" \
    --resource-group $RESOURCE_GROUP_NAME \
    --scopes /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP_NAME/providers/Microsoft.ContainerService/managedClusters/$AKS_CLUSTER_NAME \
    --condition "avg Percentage CPU > 80" \
    --description "AKS cluster CPU usage is high"
```

#### 11.2 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¼·åŒ–
```yaml
# æ”¹å–„ã•ã‚ŒãŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä¾‹
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’å‚è€ƒã«ã€å•é¡Œã®ç‰¹å®šã¨è§£æ±ºã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«è©³ç´°ãªæ”¯æ´ãŒå¿…è¦ãªå ´åˆã¯ã€Azure ã‚µãƒãƒ¼ãƒˆã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚
